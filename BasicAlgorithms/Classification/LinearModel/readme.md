线性linear，指量与量之间按比例、成直线的关系，在空间和时间上代表规则和光滑的运动，一阶导数为常数；非线性non-linear则指不按比例、不成直线的关系，代表不规则的运动和突变，一阶导数不为常数。

1.定义
线性方程：代数方程如y = 2x + 5,其中任何一个变量都为1次幂，这种方程的图像为一条直线(平面)，所以称为线性方程
非线性方程：y**2 = 2x + 5

,因变量和自变量之间不是线性关系，如平方关系、对数关系、指数关系和三角函数关系等线性模型试图学得一个通过属性的线性组合来进行预测的函数，即
f(x) = ω1 X1 + ω2 X2 +...+ωdXd + b

一般用向量形式写成 f(x) = ωTx + b，其中ω = （ω1; ω2;...;ωd）可以认为是各属性的权重，x = (x1; x2;...;xd)

优点：可解释性强；非线性模型可以通过在线性模型的基础上引入层级结构或者高维映射而得。

总结: 特征是一维的，线性模型在二维空间构成一条直线；特征是二维的，线性模型在三维空间中构成一个平面；若特征是三维的，则最终模型在四维空间中构成一个体；以此类推。

2.线性回归

线性模型试图学得f(x) = w*x + b,使得f(x)约等于y,如何确定w和b呢？这是一个组合问题，已知一些数据，如何求里面的未知参数，给出一个最优解。这是一个线性矩阵方程，直接求解很可能无法求解，有唯一解的数据集更是微乎其微，基本上都是解不存在的超定方程组。在这种情况下，我们将参数求解问题转化为误差最小化问题，求出一个最接近的解，这就是一个松弛求解。

均方误差作为回归任务中最常用的性能度量，因此我们可以试图让均方误差最小化，即


基于均方误差最小化来进行线性模型的求解方法称为“最小二乘法”。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小。求解w和b使均方误差最小化的过程，称为线性回归模型的最小二乘“参数估计(parameterestimation)”。更一般的样本由d个属性描述，此时称为多元线性回归

第一步：把w和b吸收入向量形式w=(w,b),相应的把数据集D表示为一个m*(d+1)大小的矩阵X，其中每一行对应一个示例，改行的前d个元素对应于示例的d个属性值,最后一个元素恒置为1，当为满秩矩阵(full-rank matrix)或正定矩阵(positivedefinite matrix)时，有


然而，现实任务中往往不是满秩矩阵，例如在许多任务中我们会遇到大量的变量，其数目甚至超过样例数，导致矩阵的列数多余行数，此时可以解出多个w，他们都能使均方误差最小化，选择哪一个解作为输出，将由学习算法的归纳偏好决定，常见的做法是引入正则化。
